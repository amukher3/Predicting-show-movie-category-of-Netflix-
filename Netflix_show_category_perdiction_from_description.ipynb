{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author:Abhishek Mukherjee\n",
    "Email: abhi0787@gmail.com\n",
    "Email: amukher3@rocket.utoledo.edu\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentaries                                                       299\n",
      "Stand-Up Comedy                                                     273\n",
      "Dramas, International Movies                                        248\n",
      "Dramas, Independent Movies, International Movies                    186\n",
      "Comedies, Dramas, International Movies                              174\n",
      "                                                                   ... \n",
      "Reality TV, Science & Nature TV                                       1\n",
      "Children & Family Movies, Documentaries, Sports Movies                1\n",
      "Action & Adventure, Cult Movies, Dramas                               1\n",
      "Action & Adventure, Children & Family Movies, Independent Movies      1\n",
      "Action & Adventure, Comedies, Music & Musicals                        1\n",
      "Name: listed_in, Length: 461, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"C:/Users/abhi0/Downloads/netflix_titles.csv\")\n",
    "\n",
    "def dataCleaninig(df):\n",
    "    #Original show categories(non-unique):\n",
    "    #A list to iterate over: \n",
    "    showCat=list(df['listed_in'])\n",
    "    #Description for the shows\n",
    "    showDes=list(df['description'])\n",
    "    Idx=[]\n",
    "\n",
    "    #Number of original categories(unique): \n",
    "    oriCat= list(df['listed_in'].unique())\n",
    "    numOriCat=len(oriCat)\n",
    "    \n",
    "    #A histogram for the count of\n",
    "    #different categories;\n",
    "    #plt.hist(showCat,bins=numOriCat)\n",
    "\n",
    "    #Counting the categories:\n",
    "    print(df['listed_in'].value_counts())\n",
    "\n",
    "    #Major categories to be included: \n",
    "    #Children/Family/Kids,Comedy,International,korean,British,Spanish,Sci-Fi/Fantasy,\n",
    "    #Thriller,Dramas,Horror,Romantic,Musical/Music,Anime,Documentaries,Science and Nature,\n",
    "    #Mysteries,crime,Cult,Talk shows,sports,Classic,LGBTQ,Independent,Teen,Docuseries,\n",
    "    #Faith & Spirituality. \n",
    "\n",
    "    #Categories to be included: \n",
    "    categories=['Documentaries',\n",
    "                'Comedy|Comedies',\n",
    "                'International',\n",
    "                'Independent',\n",
    "                'Children|children|Childrens|childrens|Family|family|Kids|kids',\n",
    "                'Korean|korean',\n",
    "                'British|british',\n",
    "                'Spanish|spanish',\n",
    "                'Sci-Fi|sci-fi',\n",
    "                'Fantasy|fantasy',\n",
    "                'Thriller|thriller',\n",
    "                'Dramas|dramas',\n",
    "                'Horror|horror',\n",
    "                'Romantic|romantic',\n",
    "                'Musical|musical|Music|music',\n",
    "                'Anime|anime',\n",
    "                'Science and Nature|science and nature',\n",
    "                'Mysteries|mysteries',\n",
    "                'crime|Crime',\n",
    "                'Cult|cult',\n",
    "                'Talk shows',\n",
    "                'sports|Sports',\n",
    "                'Classic|classic',\n",
    "                'LGBTQ',\n",
    "                'Teen|teen',\n",
    "                'Docuseries|docuseries',\n",
    "                'Faith & Spirituality']\n",
    "\n",
    "    labeledCategories=['Documentaries',\n",
    "                       'Comedy',\n",
    "                       'International',\n",
    "                       'Independent',\n",
    "                       'Children and Kids',\n",
    "                       'Korean',\n",
    "                       'British',\n",
    "                       'Spanish',\n",
    "                       'Sci-Fi',\n",
    "                       'Fantasy',\n",
    "                       'Thriller',\n",
    "                       'Dramas',\n",
    "                       'Horror',\n",
    "                       'Romantic',\n",
    "                       'Musical',\n",
    "                       'Anime',\n",
    "                       'Science and Nature',\n",
    "                       'Mysteries',\n",
    "                       'Crime',\n",
    "                       'Cult',\n",
    "                       'Talk shows',\n",
    "                       'Sports',\n",
    "                       'Classic',\n",
    "                       'LGBTQ',\n",
    "                       'Teen',\n",
    "                       'Docuseries',\n",
    "                       'Faith & Spirituality']\n",
    "\n",
    "    for i in range(len(showCat)):\n",
    "        Flag='False'  \n",
    "        for j in range(len(categories)):\n",
    "            if(re.search(categories[j],showCat[i])):\n",
    "                Idx.append(j)\n",
    "                showCat[i]=labeledCategories[j]\n",
    "                Flag='True'\n",
    "        if Flag!='True':\n",
    "            showCat[i]='Misc. Category'\n",
    "        \n",
    "    ##plottting the histogram after reducing the labels:\n",
    "    #plt.figure()        \n",
    "    #plt.hist(showCat,bins=len(set(showCat)))    \n",
    "    \n",
    "    showDes=pd.Series(showDes)\n",
    "    showCat=pd.Series(showCat)\n",
    "\n",
    "    df_Final=pd.concat([showDes,showCat],axis=1)\n",
    "\n",
    "    df_Final.columns=['Description','Category']\n",
    "    \n",
    "    \n",
    "    return df_Final,labeledCategories\n",
    "\n",
    "df_Final,labeledCategories=dataCleaninig(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(df_Final['Category'])\n",
    "\n",
    "# one hot encode\n",
    "encoded_labels = to_categorical(integer_encoded)\n",
    "print(encoded_labels)\n",
    "print(type(encoded_labels))\n",
    "\n",
    "\n",
    "#Labels encoded finally in the labels\n",
    "#due to intersection of words some labels\n",
    "#would be taken in the other category. \n",
    "#Hence, we start with the category with \n",
    "#highest number. \n",
    "\n",
    "\n",
    "numCat_final=len(df_Final['Category'].unique())\n",
    "print(numCat_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190, 173, 440, 183, 360, 308, 302, 119, 442, 492, 46, 355, 258, 119, 60, 175, 334, 401, 456, 471, 372, 289]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = df_Final['Description']\n",
    "labels = encoded_labels\n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(docs, labels , test_size = 0.20)\n",
    "\n",
    "vocab_size = 500\n",
    "\n",
    "X_train = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_train]\n",
    "X_test = [one_hot(d, vocab_size,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True, split=' ') for d in X_test]\n",
    "\n",
    "\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='pre')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='pre')\n",
    "print(len(labeledCategories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 300)          150000    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               15360512  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 22)                5654      \n",
      "=================================================================\n",
      "Total params: 15,647,494\n",
      "Trainable params: 15,647,494\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhi0\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3490 samples, validate on 1497 samples\n",
      "Epoch 1/10\n",
      "3490/3490 [==============================] - 46s 13ms/step - loss: 0.1319 - acc: 0.9536 - val_loss: 0.1266 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12656, saving model to C:/Users/abhi0/OneDrive/Documents/show_category._prediction_from_show_description/BestModel.h5\n",
      "Epoch 2/10\n",
      "3490/3490 [==============================] - 26s 7ms/step - loss: 0.1059 - acc: 0.9619 - val_loss: 0.1303 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.12656\n",
      "Epoch 3/10\n",
      "3490/3490 [==============================] - 40s 12ms/step - loss: 0.0599 - acc: 0.9796 - val_loss: 0.1585 - val_acc: 0.9511\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12656\n",
      "Epoch 4/10\n",
      "3490/3490 [==============================] - 40s 11ms/step - loss: 0.0210 - acc: 0.9934 - val_loss: 0.2212 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12656\n",
      "Epoch 5/10\n",
      "3490/3490 [==============================] - 43s 12ms/step - loss: 0.0040 - acc: 0.9992 - val_loss: 0.2602 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12656\n",
      "Epoch 6/10\n",
      "3490/3490 [==============================] - 47s 14ms/step - loss: 9.3134e-04 - acc: 0.9999 - val_loss: 0.2862 - val_acc: 0.9441\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12656\n",
      "Epoch 7/10\n",
      "3490/3490 [==============================] - 45s 13ms/step - loss: 3.2094e-04 - acc: 1.0000 - val_loss: 0.2956 - val_acc: 0.9439\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12656\n",
      "Epoch 8/10\n",
      "3490/3490 [==============================] - 42s 12ms/step - loss: 1.4298e-04 - acc: 1.0000 - val_loss: 0.3087 - val_acc: 0.9438\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12656\n",
      "Epoch 9/10\n",
      "3490/3490 [==============================] - 28s 8ms/step - loss: 7.4328e-05 - acc: 1.0000 - val_loss: 0.3166 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12656\n",
      "Epoch 10/10\n",
      "3490/3490 [==============================] - 41s 12ms/step - loss: 4.1391e-05 - acc: 1.0000 - val_loss: 0.3255 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14c2e9390c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(numCat_final, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "mdl_chk=\\\n",
    "ModelCheckpoint('C:/Users/abhi0/OneDrive/Documents/show_category._prediction_from_show_description/BestModel.h5',\\\n",
    "                monitor='val_loss',\\\n",
    "                verbose=1,\\\n",
    "                save_best_only=True,\\\n",
    "                save_weights_only=True)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10,validation_split=0.3, callbacks=[mdl_chk],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'' was not found in history, as a file, url, nor in the user namespace.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
